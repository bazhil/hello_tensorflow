{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к файлу\n",
    "path = 'C:\\\\Users\\home\\\\Downloads\\\\Ефремов_-_Лезвие_бритвы.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем файл на чтение\n",
    "text = open(path, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина текста: 1316322 символов.\n"
     ]
    }
   ],
   "source": [
    "# Печатаем длину строки в символах\n",
    "print('Длина текста: {} символов.'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Иван Ефремов\n",
      "\n",
      "Лезвие бритвы\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Пролог\n",
      "\n",
      "\n",
      "Все быстрее нарастает познание в современном мире. Обрисовывается точнейшая взаимосвязь, обусловленность кажущихся различными явлений мира и жизни. Всеобщее переплетение отдаленных случайностей, вырастающее \n"
     ]
    }
   ],
   "source": [
    "# Печатаем первые 250 символов\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 уникальных символа\n"
     ]
    }
   ],
   "source": [
    "# Получаем уникальные символы в строке\n",
    "vocab = sorted(set(text))\n",
    "print('{} уникальных символа'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизуем текст. Получаем числовое представление каждого слова\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    '\\n':   0\n",
      "    ' ' :   1\n",
      "    '!' :   2\n",
      "    '(' :   3\n",
      "    ')' :   4\n",
      "    ',' :   5\n",
      "    '-' :   6\n",
      "    '.' :   7\n",
      "    '/' :   8\n",
      "    '0' :   9\n",
      "    '1' :  10\n",
      "    '2' :  11\n",
      "    '3' :  12\n",
      "    '4' :  13\n",
      "    '5' :  14\n",
      "    '6' :  15\n",
      "    '7' :  16\n",
      "    '8' :  17\n",
      "    '9' :  18\n",
      "    ':' :  19\n",
      "   ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Выводим числовое представление для первых 20 символов\n",
    "print('{')\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('    {:4s}: {:3d}'.format(repr(char), char2idx[char]))\n",
    "print('   ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n\\nИван Ефремов\\n\\nЛезв' ---- символов преобразовано в числа ----> [ 0  0 56 79 77 90  1 53 97 93 82 89 91]\n"
     ]
    }
   ],
   "source": [
    "# Выводим первые 20 символов текста, представленные в виде чисел\n",
    "print('{} ---- символов преобразовано в числа ----> {}'.format(repr(text[:20]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "И\n",
      "в\n",
      "а\n",
      "н\n",
      " \n",
      "Е\n",
      "ф\n",
      "р\n",
      "е\n",
      "м\n",
      "о\n",
      "в\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Л\n",
      "е\n",
      "з\n",
      "в\n"
     ]
    }
   ],
   "source": [
    "# Максимальная длина предложения необходима для одного ввода символов \n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Создаем тренировочные примеры и цели\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(20):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n\\nИван Ефремов\\n\\nЛезвие бритвы\\n\\n\\n\\n\\nПролог\\n\\n\\nВсе быстрее нарастает познание в современном мире. Обрисов'\n",
      "'ывается точнейшая взаимосвязь, обусловленность кажущихся различными явлений мира и жизни. Всеобщее пе'\n",
      "'реплетение отдаленных случайностей, вырастающее в необходимость, то есть в законы природы, пожалуй, с'\n",
      "'амое важное прозрение современного человека.\\n\\nИ в человеческом существовании незаметные совпадения, д'\n",
      "'авно наметившиеся сцепления обстоятельств, тонкие нити, соединяющие те или другие случайности, выраст'\n",
      "'ают в накрепко спаянную логическую цепь, влекущую за собой попавшие в ее орбиту человеческие жизни. М'\n",
      "'ы, не зная достаточно глубоко причинную связь, не понимая истинных мотивов, называем это судьбой.\\n\\nЕс'\n",
      "'ли проследить всю цепь, а затем распутать начальные ее нити, можно прийти к некоему отправному момент'\n",
      "'у, послужившему как бы спусковым крючком или замыкающей кнопкой. Отсюда начинается долгий ряд событий'\n",
      "', неизбежно долженствующих сблизить совершенно чужих людей, живущих в разных местах нашей планеты, и '\n",
      "'заставить их действовать совместно, враждуя или дружа, любя или ненавидя, в общих исканиях одной и то'\n",
      "'й же цели.\\n\\n5 марта 1916 года в Петрограде, на Морской, открылась выставка известного художника и юве'\n",
      "'лира, собирателя самоцветных сокровищ Урала Алексея Козьмича Денисова-Уральского.\\n\\nЕще внизу, в гарде'\n",
      "'робной, где суетились, угодливо кланяясь, слуги, веяло слабым ароматом французских духов и проплывали'\n",
      "', шелестя тугими платьями, дамы, можно было заключить, что выставка пользуется успехом. «Речь» и «Пет'\n",
      "'роградские ведомости» одобрили «патриотическое художество», посещение выставки стало считаться в стол'\n",
      "'ичном «свете» тоже патриотичным.\\n\\nНизкие залы казались пустоватыми и неуютными в тусклом свете пасмур'\n",
      "'ного петроградского дня. В центре каждой комнаты стояли одна-две стеклянные витрины с небольшими скул'\n",
      "'ьптурными группами, вырезанными из лучших уральских самоцветов. Камни излучали собственный свет, неза'\n",
      "'висимый от капризов погоды и темноты человеческого жилья.\\n\\nХудощавый молодой инженер в парадном сюрту'\n"
     ]
    }
   ],
   "source": [
    "# Выводим текст строками (типа предложениями по 100 символов)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sequences.take(20):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем функцию, возвращающую каждое предложение в форме для ввода\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\n\\nИван Ефремов\\n\\nЛезвие бритвы\\n\\n\\n\\n\\nПролог\\n\\n\\nВсе быстрее нарастает познание в современном мире. Обрисо'\n",
      "Target data:  '\\nИван Ефремов\\n\\nЛезвие бритвы\\n\\n\\n\\n\\nПролог\\n\\n\\nВсе быстрее нарастает познание в современном мире. Обрисов'\n"
     ]
    }
   ],
   "source": [
    "# Печатаем первые примеры\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "    input: 0 ('\\n')\n",
      "    expected output: 0 ('\\n')\n",
      "Step    1\n",
      "    input: 0 ('\\n')\n",
      "    expected output: 0 ('И')\n",
      "Step    2\n",
      "    input: 56 ('И')\n",
      "    expected output: 56 ('в')\n",
      "Step    3\n",
      "    input: 79 ('в')\n",
      "    expected output: 79 ('а')\n",
      "Step    4\n",
      "    input: 77 ('а')\n",
      "    expected output: 77 ('н')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:4d}'.format(i))\n",
    "    print('    input: {} ({:s})'.format(input_idx, repr(idx2char[input_idx])))\n",
    "    print('    expected output: {} ({:2})'.format(input_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаем тренировочные пачки\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим модель\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем функцию, которая возвращает модель\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "           return_sequences=True,\n",
    "           recurrent_initializer='glorot_uniform',\n",
    "           stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель\n",
    "model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 114) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Испытываем модель\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           29184     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 114)           116850    \n",
      "=================================================================\n",
      "Total params: 4,081,266\n",
      "Trainable params: 4,081,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 91,  86,   7,  55,  15,  40,  15, 110,  42,  47,  55, 108, 107,\n",
       "        89,   2,  86,  65,  94,  22,  50,  51,  55,  13,  37, 110,   8,\n",
       "        19,  54,  88,  48,  75,  63,  31,   0,  88,  49,  28,  27, 101,\n",
       "        10, 113,  33,  90,  43,  98,  31, 101,   8,  87, 107, 100,  79,\n",
       "        48, 107,  77,  45,   9, 102,   1,  71,  28,  79,  80,  16,  51,\n",
       "       110, 113,  72,  76,  59, 108,  83,  57,  38,  28,  16,  97,   2,\n",
       "        33,  31,  45, 105,  98,  99,  74,  32,  25,  11,  31,  10,  32,\n",
       "        62,   5,  36,  20,  51,  92,  60, 103,  36], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " 'росила Сима.\\n\\n–\\xa0Вероятно, в дальнейшем наука будет способна и не на такие чудеса. Действительно, все'\n",
      "\n",
      "Next char Predictions: \n",
      " 'ой.З6r6–t»Зяюм!йСсIВГЗ4n–/:ЖлАЮПg\\nлБcbш1…iнuхgш/кючвАюа\\xa00щ Чcвг7Г–…ШЯЛяжЙoc7ф!ig\\xa0ьхцЭhX2g1hО,m;ГпМъm'\n"
     ]
    }
   ],
   "source": [
    "# Выводим предсказание нетренированной модели\n",
    "print('input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print('Next char Predictions: \\n', repr(''.join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predication shape:  (64, 100, 114)  # (batch_size, sequence_length, vocab_size)\n",
      "Scalar_loss:        4.7374616\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Predication shape: ', example_batch_predictions.shape, ' # (batch_size, sequence_length, vocab_size)')\n",
    "print('Scalar_loss:       ', example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем тренировочную процедуру\n",
    "model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "204/205 [============================>.] - ETA: 1:29 - loss: 2.7981WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "205/205 [==============================] - 18438s 90s/step - loss: 2.7958\n",
      "Epoch 2/3\n",
      "205/205 [==============================] - 19863s 97s/step - loss: 2.1675\n",
      "Epoch 3/3\n",
      "205/205 [==============================] - 14697s 72s/step - loss: 1.8855\n"
     ]
    }
   ],
   "source": [
    "# Выполняем тренировку\n",
    "EPOCHS = 3\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            29184     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 114)            116850    \n",
      "=================================================================\n",
      "Total params: 4,081,266\n",
      "Trainable params: 4,081,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Генерируем текст\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наука – верно, что все дочь имоевственный в десторнам, – уловья – не протрогие задерев в эте. Чида может желаной, сефень и слота дваю женщина. А пот экрулих многом, хучались необочить в орином. Гирин цердал, что онг Гирин и бящую Кимства. Якотоло этой только заделжавший цеания рукном и давнах целую! Наустал своягдушим, сейдне, Чезаре в слученную серятнице начу! – Гирин рас встовор, Нараверно? но девяшка, новикие обказание в Востою время. Чазаре. А тенного зверьяться в хаш бы пока, тло сила, собывойсонаят к яды и паттиступа постагоря. Чакая мужчанам, судала Леа.\n",
      "\n",
      "– Пойте понимаются в корОне для шехга. Умерносят крокроги двиют этах буды, невироков, что вы длянные ли.. На Аласкапльно простат тержение кону, как аякапальная охтати оставать путь часов. – Ризамисто и дюмак хидушнов Челезик… Блазым вас Гирина устудного, прозык, порожая разговой. И время эта тели, – Может пей, которые слобость? – осторожно был у на» в образрении. Сенчас сойсне этскудать взгляд.\n",
      "\n",
      "Сонаренно! – работа падральству из жив\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string=u'Наука '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
